{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJgfagqCTjZw"
   },
   "source": [
    "# The Hangman Game\n",
    "## Sayash Raaj\n",
    "### IIT Madras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGbizstGTjZy"
   },
   "source": [
    "## Instruction:\n",
    "For this project, your mission is to write an algorithm that plays the game of Hangman through our API server.\n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import secrets\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import callbacks, saving\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "try:\n",
    "    from urllib.parse import urlparse, parse_qs, urlencode\n",
    "except ImportError:\n",
    "    from urlparse import urlparse, parse_qs\n",
    "    from urllib import urlencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sayash Raaj\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "256               |256               |embedding_dim\n",
      "2                 |2                 |lstm_layers\n",
      "64                |64                |lstm_units_0\n",
      "256               |256               |lstm_units_1\n",
      "0.3               |0.3               |dropout_rate\n",
      "0.000388          |0.000388          |learning_rate\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sayash Raaj\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HangmanHyperparameterTuning:\n",
    "    \n",
    "    def __init__(self, full_dictionary, test_size=0.10):\n",
    "        self.full_dictionary = full_dictionary\n",
    "        self.max_length = 0\n",
    "        self.test_size=test_size\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        encoded_dict = [[27 if char == '_' else ord(char) - ord('a') + 1 for char in word] for word in self.full_dictionary]\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Process each encoded word\n",
    "        for word_encoding in encoded_dict:\n",
    "            char_positions = {}\n",
    "            unique_chars = set(word_encoding)\n",
    "\n",
    "            # Group character positions\n",
    "            for char in unique_chars:\n",
    "                char_positions[char] = [i for i, c in enumerate(word_encoding) if c == char]\n",
    "\n",
    "            # Create masked words and append to X and y\n",
    "            for char, positions in char_positions.items():\n",
    "                masked_word = word_encoding[:]\n",
    "                for pos in positions:\n",
    "                    masked_word[pos] = 27\n",
    "                X.append(masked_word + [len(masked_word)] * 2)  # Add two placeholders\n",
    "                y.append(char - 1)\n",
    "\n",
    "        # Update max_length and adjust X\n",
    "        for i, sample in enumerate(X):\n",
    "            X[i] = sample[:-2]\n",
    "            self.max_length = max(self.max_length, len(sample) - 2)\n",
    "\n",
    "        # Pad sequences for consistent length\n",
    "        X = pad_sequences(X, maxlen=self.max_length, padding='post')\n",
    "\n",
    "        # Convert y to categorical labels\n",
    "        y = to_categorical(y, num_classes=26)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        return train_test_split(X, y, test_size=self.test_size, random_state=42)\n",
    "    \n",
    "    def build_model(self, hp):\n",
    "        model=Sequential()\n",
    "        \n",
    "        embedding_dim = hp.Int('embedding_dim', min_value=64, max_value=256, step=64)\n",
    "        model.add(Embedding(28,embedding_dim, input_length=self.max_length))\n",
    "        \n",
    "        for i in range(hp.Int('lstm_layers', 2, 5)):\n",
    "            units = hp.Int(f'lstm_units_{i}', min_value=64, max_value=256, step=64)\n",
    "            return_sequences = i < (hp.Int('lstm_layers', 2, 5)-1)\n",
    "            model.add(Bidirectional(LSTM(units, return_sequences=return_sequences)))\n",
    "            \n",
    "        model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
    "        \n",
    "        model.add(Dense(26, activation='softmax'))\n",
    "        \n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def tune_model(self):\n",
    "        tuner = kt.RandomSearch(\n",
    "            self.build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=5,\n",
    "            executions_per_trial=1,\n",
    "            directory='hyperparam_tuning_random',\n",
    "            project_name='hangman_random_search'\n",
    "        )\n",
    "        \n",
    "        stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        tuner.search(self.train_x,self.train_y,epochs=2,validation_data=(self.test_x,self.test_y), callbacks=[stop_early])\n",
    "        \n",
    "        best_hps = tuner.get_best_hyperparameters()[0]\n",
    "        return best_hps\n",
    "    \n",
    "full_dictionary = open(\"words.txt\").read().splitlines()\n",
    "hangman_tuner = HangmanHyperparameterTuning(full_dictionary)\n",
    "best_hps = hangman_tuner.tune_model()\n",
    "\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters found:\n",
    "embedding_dim: 128,<br>\n",
    "lstm_layers: 2,<br>\n",
    "lstm_units_0: 256,<br>\n",
    "lstm_units_1: 256,<br>\n",
    "dropout_rate: 0.2,<br>\n",
    "learning_rate: 0.0006029308510873298,<br>\n",
    "lstm_units_2: 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model, Guessing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rj9RdA1pTjZ-"
   },
   "outputs": [],
   "source": [
    "hangman_url = \"https://trexsim.com\"\n",
    "\n",
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None, batch_size=1000, epochs=25, test_size=0.10, already_trained=True):\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.test_size = test_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.guessed_letters = []\n",
    "        self.full_dictionary = self.build_dictionary(\"words.txt\")\n",
    "        self.max_length = 0\n",
    "        self.already_trained = already_trained\n",
    "        self.build_and_train_model()  # three-layer bidirectional LSTM\n",
    "\n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "        data = {}\n",
    "\n",
    "        for link in links:\n",
    "            for _ in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = min(data, key=data.get)\n",
    "        return link + '/trexsim/hangman'\n",
    "\n",
    "    def guess(self, word):\n",
    "        filtered_word = word[::2]\n",
    "        encoded_word = [27 if char == '_' else ord(char) - ord('a') + 1 for char in filtered_word]\n",
    "        padded_sequence = pad_sequences([encoded_word], maxlen=self.max_length, padding='post')\n",
    "        prediction = self.model.predict(padded_sequence)\n",
    "        letter_order = [chr(i + ord('a')) for i in sorted(range(len(prediction[0])), key=lambda i: prediction[0][i], reverse=True)]\n",
    "\n",
    "        for letter in letter_order:\n",
    "            if letter not in self.guessed_letters:\n",
    "                return letter\n",
    "\n",
    "    def process_dictionary(self):\n",
    "        word_data = [[27 if char == '_' else ord(char) - ord('a') + 1 for char in word] for word in self.full_dictionary]\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        for word_code in word_data:\n",
    "            letter_positions = {char: [] for char in set(word_code)}\n",
    "\n",
    "            for idx, char in enumerate(word_code):\n",
    "                letter_positions[char].append(idx)\n",
    "\n",
    "            for char, positions in letter_positions.items():\n",
    "                masked_word = word_code[:]\n",
    "\n",
    "                for pos in positions:\n",
    "                    masked_word[pos] = 27\n",
    "\n",
    "                target = char - 1\n",
    "                X.append(masked_word + [0, 0])\n",
    "                y.append(target)\n",
    "\n",
    "        self.max_length = max(map(len, X))\n",
    "        X = pad_sequences(X, maxlen=self.max_length, padding='post')\n",
    "        y = to_categorical(y, num_classes=26)\n",
    "\n",
    "        return train_test_split(X, y, test_size=self.test_size, random_state=42)\n",
    "\n",
    "    def build_and_train_model(self):\n",
    "        train_x, test_x, train_y, test_y = self.process_dictionary()\n",
    "\n",
    "        self.model = Sequential([\n",
    "            Embedding(input_dim=28, output_dim=128, input_length=self.max_length),\n",
    "            Bidirectional(LSTM(256, return_sequences=True)),\n",
    "            Bidirectional(LSTM(256, return_sequences=True)),\n",
    "            Bidirectional(LSTM(256)),\n",
    "            Dropout(0.2),\n",
    "            Dense(26, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        custom_optimizer = keras.optimizers.Adam(learning_rate=0.0006029308510873298)\n",
    "        self.model.compile(optimizer=custom_optimizer, loss=\"categorical_crossentropy\", metrics=['top_k_categorical_accuracy', 'accuracy'])\n",
    "\n",
    "        checkpoint_filepath = 'checkpoint.model.keras'\n",
    "        model_checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "        if not self.already_trained:\n",
    "            self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(test_x, test_y), callbacks=[model_checkpoint])\n",
    "            self.model.save('model.keras')\n",
    "        else:\n",
    "            self.model = keras.models.load_model('model.keras')\n",
    "\n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        with open(dictionary_file_location, \"r\") as text_file:\n",
    "            return text_file.read().splitlines()\n",
    "\n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        self.guessed_letters = []\n",
    "\n",
    "        response = self.request(\"/new_game\", {\"practice\": practice})\n",
    "        if response.get('status') == \"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(f\"Successfully start a new game! Game ID: {game_id}. # of tries remaining: {tries_remains}. Word: {word}.\")\n",
    "            while tries_remains > 0:\n",
    "                guess_letter = self.guess(word)\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(f\"Guessing letter: {guess_letter}\")\n",
    "\n",
    "                try:\n",
    "                    res = self.request(\"/guess_letter\", {\"request\": \"guess_letter\", \"game_id\": game_id, \"letter\": guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Server response: {res}\")\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status == \"success\":\n",
    "                    if verbose:\n",
    "                        print(f\"Successfully finished game: {game_id}\")\n",
    "                    return True\n",
    "                elif status == \"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(f\"Failed game: {game_id}. Because of: {reason}\")\n",
    "                    return False\n",
    "                elif status == \"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status == \"success\"\n",
    "\n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "\n",
    "    def request(self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = {}\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        if self.access_token:\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for _ in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception during request: {str(e)}\")\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            content_type = response.headers.get(\"content-type\")\n",
    "            if content_type.startswith(\"text\"):\n",
    "                return response.text\n",
    "            if content_type.startswith(\"application/json\"):\n",
    "                return json.loads(response.content.decode(\"utf-8\"))\n",
    "            return response.content\n",
    "        elif 400 <= response.status_code < 500:\n",
    "            raise HangmanAPIError(response)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY3jFZ2mTjaF"
   },
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrvI936lTjaH"
   },
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object.\n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpxcNE2uTjaI",
    "outputId": "fff06735-abc3-42f2-eabf-8a9fbed7516e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m  2/505\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:29:39\u001b[0m 32s/step - accuracy: 0.0402 - loss: 3.2532 - top_k_categorical_accuracy: 0.1777"
     ]
    }
   ],
   "source": [
    "api = HangmanAPI(access_token=\"1db5838f288d84cd4dec2db9be95ab\", timeout = 2000, batch_size = 3000, epochs = 5, test_size = 0.1, already_trained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpTO-3yQTjaQ"
   },
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_sBe4A_7OfKk",
    "outputId": "31b36d6d-95d6-4853-a4c0-547d017bfc2a"
   },
   "outputs": [],
   "source": [
    "api.start_game(practice=1,verbose=True)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm27-Nb9Tjad"
   },
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hge6f53sTjam",
    "outputId": "ad8944ec-2bc8-48f9-d762-16528e25ee6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    #api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJAB2z_PTjaq"
   },
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWZ0fO2DTPLm",
    "outputId": "4a1f4e1e-3239-4197-eea6-f7d255e51eaf"
   },
   "outputs": [],
   "source": [
    "api.my_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJLA_zS54_uC",
    "outputId": "3d1ca35a-cdb7-4a81-bef9-80e519ec6777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.607\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Success Rate: 60.7%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
